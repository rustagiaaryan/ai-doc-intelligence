# FILE: services/llm-proxy/.env.example

# Service Configuration
SERVICE_NAME=llm-proxy
PORT=8002
ENV=development

# LLM Provider API Keys
OPENAI_API_KEY=sk-your-openai-api-key
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key

# Default LLM Configuration
DEFAULT_PROVIDER=openai
DEFAULT_MODEL=gpt-4-turbo-preview
DEFAULT_EMBEDDING_MODEL=text-embedding-3-small
DEFAULT_MAX_TOKENS=4096
DEFAULT_TEMPERATURE=0.7

# Redis (for caching)
REDIS_URL=redis://redis:6379/1
CACHE_TTL_SECONDS=3600

# Rate Limiting
RATE_LIMIT_REQUESTS_PER_MINUTE=60

# CORS (JSON array format)
CORS_ORIGINS=["http://localhost:3000","http://localhost:8080"]
